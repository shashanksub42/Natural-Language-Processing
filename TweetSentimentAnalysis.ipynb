{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "Dataset loaded with shape:  (1600000, 3)\n",
      "\n",
      "Processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1280000it [00:06, 186405.75it/s]\n",
      "320000it [00:02, 145468.10it/s]\n",
      "100%|██████████| 1280000/1280000 [00:00<00:00, 2168906.37it/s]\n",
      "100%|██████████| 1280000/1280000 [00:00<00:00, 2214349.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to  vagina :  [('uterus', 0.687721848487854), ('eyelid', 0.6832553148269653), ('armpit', 0.6534185409545898), (\"dog's\", 0.6396236419677734), ('wrists', 0.6336432099342346), ('penis', 0.6291688680648804), ('cheeks', 0.6285846829414368), ('eyeball', 0.6276131868362427), ('lips', 0.6256489157676697), ('scar', 0.6247669458389282)]\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 5000 samples in 0.027s...\n",
      "[t-SNE] Computed neighbors for 5000 samples in 7.313s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 5000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 5000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 5000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 5000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 5000\n",
      "[t-SNE] Mean sigma: 0.570439\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 84.641594\n",
      "[t-SNE] Error after 1000 iterations: 2.243080\n",
      "\n",
      "Fitting training data using Multinomial Naïve Bayes...\n",
      "\n",
      "Model fitting done.\n",
      "\n",
      "Accuracy of Multinomial Naïve Bayes model:  77.3434375 %\n",
      "\n",
      "Fitting training data using SGDClassifier...\n",
      "\n",
      "Model fitting done.\n",
      "\n",
      "Accuracy of SGD Classifier model:  73.173125 %\n",
      "\n",
      "Fitting training data using GridSearchCV...\n",
      "\n",
      "Model fitting done.\n",
      "\n",
      "Accuracy of GridSearchCV model:  73.190078125 %\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################################################################\n",
    "\n",
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from copy import deepcopy \n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "################################################################################################################################################################################\n",
    "\n",
    "#Definitions\n",
    "\n",
    "def ingest():\n",
    "    temp = []\n",
    "    data = pd.read_csv('/Users/anmolukhare/Downloads/trainingandtestdata/training.1600000.processed.noemoticon.csv', index_col = None, encoding = 'latin-1')\n",
    "    data.columns = ['Sentiment', 'ItemID', 'TimeStamp', 'Query', 'SentimentSource', 'SentimentText']\n",
    "    data.drop(['ItemID', 'TimeStamp', 'Query', 'SentimentSource'], axis = 1, inplace = True)\n",
    "    data = data[data.Sentiment.isnull() == False]\n",
    "    data['Sentiment'] = data['Sentiment'].map({4:1, 0:0})\n",
    "    data = data[data['SentimentText'].isnull() == False]\n",
    "    data.reset_index(inplace = True)\n",
    "    temp.insert(0, {'Sentiment':'0', 'SentimentText':'@switchfoot http://twitpic.com/2y1zl - Awww, that\\'s a bummer. You shoulda got David Carr of Third Day to do it. ;D'})\n",
    "    data = pd.concat([pd.DataFrame(temp), data], ignore_index = True)\n",
    "    print('\\nDataset loaded with shape: ', data.shape)\n",
    "    return data\n",
    "\n",
    "def WordToVec(data, n, n_dim):\n",
    "    def tokenize(tweet):\n",
    "        try:\n",
    "            tweet = unicode(tweet.decode('utf-8').lower())\n",
    "            tokens = tokenizer.tokenize(tweet)\n",
    "            tokens = filter(lambda t: not t.startswith('@'), tokens)\n",
    "            tokens = filter(lambda t: not t.startswith('#'), tokens)\n",
    "            tokens = filter(lambda t: not t.startswith('http'), tokens)\n",
    "            return tokens\n",
    "        except:\n",
    "            return 'NC'\n",
    "\n",
    "\n",
    "    def postprocess(data, n=1600000):\n",
    "        data = data.head(n)\n",
    "        data['tokens'] = data['SentimentText'].apply(tokenizer.tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "        data = data[data.tokens != 'NC']\n",
    "        data.reset_index(inplace=True)\n",
    "        data.drop('index', inplace=True, axis=1)\n",
    "        return data\n",
    "\n",
    "    print(\"\\nProcessing data...\")\n",
    "    data = postprocess(data)\n",
    "\n",
    "    #Splitting into training and testing data\n",
    "    n = 1600000\n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens), np.array(data.head(n).Sentiment), test_size=0.2)\n",
    "\n",
    "    def labelizeTweets(tweets, label_type):\n",
    "        labelized = []\n",
    "        for i,v in tqdm(enumerate(tweets)):\n",
    "            label = '%s_%s'%(label_type,i)\n",
    "            labelized.append(LabeledSentence(v, [label]))\n",
    "        return labelized\n",
    "\n",
    "    x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "    x_test = labelizeTweets(x_test, 'TEST')\n",
    "\n",
    "\n",
    "    tweet_w2v = Word2Vec(size=200x, min_count=10)\n",
    "    tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "    tweet_w2v.train([x.words for x in tqdm(x_train)], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)\n",
    "    \n",
    "    temp = input(\"Enter a word to check most similarity matches: \")\n",
    "    print(\"Most similar to \", temp, \": \", tweet_w2v.most_similar(temp))\n",
    "\n",
    "    # getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "    word_vectors = [tweet_w2v[w] for w in list(tweet_w2v.wv.vocab.keys())[:5000]]\n",
    "\n",
    "    # dimensionality reduction. converting the vectors to 2d vectors\n",
    "    from sklearn.manifold import TSNE\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "    tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "    # putting everything in a dataframe\n",
    "    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "    tsne_df['words'] = list(tweet_w2v.wv.vocab.keys())[:5000]\n",
    "    return\n",
    "\n",
    "\n",
    "def Pre_process(data):\n",
    "    X = data.SentimentText\n",
    "    Y = data.Sentiment\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def NaiveBayes(data):\n",
    "    X_train, X_test, Y_train, Y_test = Pre_process(data)\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', naive_bayes.MultinomialNB())])\n",
    "    print(\"\\nFitting training data using Multinomial Naïve Bayes...\")\n",
    "    text_clf.fit(X_train, np.asarray(Y_train, dtype = np.float64))  \n",
    "    print(\"\\nModel fitting done.\")\n",
    "    pred = text_clf.predict(X_test)\n",
    "    print(\"\\nAccuracy of Multinomial Naïve Bayes model: \", np.mean(pred == Y_test)*100, \"%\")\n",
    "    return\n",
    "\n",
    "def SGD_Classifier(data):\n",
    "    X_train, X_test, Y_train, Y_test = Pre_process(data)\n",
    "    text_clf1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))])\n",
    "    print(\"\\nFitting training data using SGDClassifier...\")\n",
    "    text_clf1.fit(X_train, np.asarray(Y_train, dtype = np.float64))\n",
    "    filename = 'trained_model.sav'\n",
    "    pickle.dump(text_clf1, open(filename, 'wb'))\n",
    "    print(\"\\nModel fitting done.\")\n",
    "    pred1 = text_clf1.predict(X_test)\n",
    "    print(\"\\nAccuracy of SGD Classifier model: \", np.mean(pred1 == Y_test)*100, \"%\")\n",
    "    return\n",
    "\n",
    "def Grid_Search_CV(data):\n",
    "    X_train, X_test, Y_train, Y_test = Pre_process(data)\n",
    "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}\n",
    "    filename = 'trained_model.sav'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    gs_clf = GridSearchCV(loaded_model, parameters, n_jobs=-1)\n",
    "    print(\"\\nFitting training data using GridSearchCV...\")\n",
    "    gs_clf = gs_clf.fit(X_train[:1600000], np.asarray(Y_train[:1600000], dtype = np.float64))\n",
    "    print(\"\\nModel fitting done.\")\n",
    "    print(\"\\nAccuracy of GridSearchCV model: \", gs_clf.best_score_*100, \"%\") \n",
    "    return\n",
    "\n",
    "################################################################################################################################################################################\n",
    "\n",
    "#Code\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "data = ingest()\n",
    "data.drop(['index'], axis = 1, inplace = True)\n",
    "n=1000000\n",
    "n_dim = 200\n",
    "WordToVec(data, n, n_dim)\n",
    "NaiveBayes(data)\n",
    "SGD_Classifier(data)\n",
    "Grid_Search_CV(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
